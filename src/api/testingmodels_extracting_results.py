# -*- coding: utf-8 -*-
"""TestingModels_extracting_results.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Bz4Ne7kjWwXJZmGQ2e2th_vwKaFKqGo6

# Library imports
"""



# Miscelanious Modules
import tensorflow as tf
import os
import sys
import inspect
import numpy as np
import tensorflow as tf
import keras as K
import sklearn as sk
import pickle
import shutil
import cv2
import json

# Objects and methods
# get_ipython().run_line_magic('matplotlib', 'inline')
from PIL import Image, ImageOps, ImageEnhance, ImageFile
import PIL.Image
from numpy import random
from IPython.display import SVG
from distutils.version import LooseVersion as LV
from sklearn.model_selection import train_test_split
from shutil import copyfile
from datetime import datetime
from tensorflow.python.framework import ops
from tensorflow.keras.preprocessing.image import load_img, img_to_array
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, classification_report, roc_curve, roc_auc_score, balanced_accuracy_score
from sklearn.utils.class_weight import compute_class_weight

# Keras objects and methods
from keras.models import Sequential,model_from_json, load_model
from keras.preprocessing import image
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from keras.layers import Dense, Activation, Dropout, Flatten, MaxPooling2D,Conv2D
from keras import backend as K
from keras import callbacks
from keras import __version__

import numpy as np
import tensorflow as tf
np.random.seed(42)
tf.random.set_seed(42)

#TensorFlow Version
print(tf.__version__)
print(tf.config.list_physical_devices('GPU'))


#Python Version using
from platform import python_version
print('Versão da Linguagem Python Usada Neste Jupyter Notebook:', python_version())

"""# Path definitions"""

# from google.colab import drive
# drive.mount('/content/drive')



#!unzip /content/drive/MyDrive/Imagens128.zip -d dataset/

# Path to directories with dry and wet images division
base_dir1 = './imgs-api'
base_dir2 = './imgs-api'

# train, validation and test sub-directories
train_dir = os.path.join(base_dir1, 'train')
validation_dir = os.path.join(base_dir1, 'validation')
test_dir = os.path.join(base_dir1, 'test')
#test_dir = os.path.join(base_dir2, 'test_holdout128')
print(base_dir1)

# exit(1)

"""# Filtering Definition"""

def suavizar(image):
    suavizada = cv2.GaussianBlur(image, (5, 5), 0)
    return suavizada

# train_datagen = ImageDataGenerator(
#    preprocessing_function=suavizar, shear_range=0.2,zoom_range=0.2, horizontal_flip=True, rescale=1./255
# )

#Pixel normalization for test data
test_datagen = ImageDataGenerator(rescale=1./255)


#Pixel normalization for validation data
validation_datagen = ImageDataGenerator(rescale=1./255)

image_size=128

# batch_size=2000


# Applying filter accross train directory with batch size = 6:
#train_generator = train_datagen.flow_from_directory(
#      train_dir,
#      target_size=(image_size,image_size), # size image
#      batch_size = batch_size, shuffle = True, class_mode='binary', classes=['dry', 'wet']
#)


# Applying filter accross train directory with batch size = 6:
# validation_generator = validation_datagen.flow_from_directory(
#      validation_dir,
#      target_size=(image_size,image_size), batch_size = 6, class_mode='binary', classes=['dry', 'wet'],
#)

# Applying filter accross test directory with batch size =1:
test_generator = test_datagen.flow_from_directory(
    test_dir,
      target_size=(image_size,image_size), batch_size = 1,  class_mode='binary', shuffle=False
)
#bbbb

# Assuming you have already created train_generator using train_datagen.flow_from_directory()

# Get the class indices (mapping of class names to their corresponding index)
class_indices = test_generator.class_indices

# Get the class labels (indices) for each image
class_labels = test_generator.classes

# Count the number of occurrences of each class label
unique, counts = np.unique(class_labels, return_counts=True)

# Create a dictionary mapping class names to their respective counts
class_counts = {class_name: counts[idx] for class_name, idx in class_indices.items()}

"""# Loading Models

## Função para lidar com imagens corrompidas
"""

def iterator_scanner(img):
    c=0
    while c<195:

        try:
            data, labels = next(img)
            yield data, labels

        except:
            pass
        c+=1

#aa
"""## Reading weights and json model files"""

# json_file = open('./models/M2.json','r')
# loaded_model_json = json_file.read()
# json_file.close()
# loaded_model = model_from_json(loaded_model_json)
# print(loaded_model)

# Leu o arquivo com os pesos iniciais do modelo:
model = load_model('./models/M1_weights_epoch_90.h5')
print('Loaded model from disk')

#Compilou o modelo:
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])


# def classificadora(img_path, model):
#     img = load_img(img_path, target_size=target_size)
#     img_array = img_to_array(img)
#     img_array = np.expand_dims(img_array, axis=0)
#     img_array = img_array / 255.0
#     return img_array

#Fez os testes
score = model.evaluate(test_generator)


print("{}: {}".format(model.metrics_names[1],score[1]*100))
print("{}: {}".format(model.metrics_names[0],score[0]*100))




#model.summary()

#Lista de chutes:
predictions = model.predict(test_generator)

threshold=0.877976
prediction_percent=(predictions*100).round(8)
predicted_labels = (predictions > threshold).astype(int) # Converter as previsões em rótulos (classes)

true_labels = test_generator.classes# Obter os rótulos verdadeiros do conjunto de teste
true_labels = true_labels[:-1]
class_names = list(test_generator.class_indices.keys())

#print(predicted_labels)

#aaa

"""# Visualizando resultado do treinamento

## Ajustando trashold e criando vizualizações
"""

#threshold=0.50392
threshold=0.877976

#threshold=0.497
prediction_percent=(predictions*100).round(8)
predicted_labels = (predictions > threshold).astype(int) # Converter as previsões em rótulos (classes)
true_labels = test_generator.classes# Obter os rótulos verdadeiros do conjunto de teste
true_labels = true_labels[:-1]
class_names = list(test_generator.class_indices.keys())

print(predictions)
print(test_generator.filenames)


print(list(test_generator.filenames))

for pred, file in zip(predictions, test_generator.filenames):
    print(f"{file} ==> {pred}")


#Objetivo: Criar uma função que

# Receba:
    # O path de uma imagem
    # um modelo carregado

# Retorne:
    # O nome da imagem carregada
    # Se ela é wet, dry. 
    # Porcentagem de chute.



def classificadora(path, model):

    resize_output = path[:-4] + '_128x128_' + path[-4:]
    with Image.open(path) as picture:
        resized_pic = picture.resize((128, 128), Image.LANCZOS)
        resized_pic.save(resize_output)

    sample = img_to_array(load_img(path=resize_output, color_mode="rgb", target_size=None))
    processing_img = np.expand_dims(sample, axis=0)
    processed_image = processing_img/255

    prediction = model.predict(processed_image)

    return [path, prediction[0][0], "wet" if (prediction[0] > threshold)[0] else "dry" ]

print(classificadora('./imgs-api/test/wet/istockphoto-183808583-612x612.jpg',model))






